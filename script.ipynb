{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalized Practice\n",
    "### *Leveraging Collaborative Filtering for Personalized Practice in Computer-Based Assessments*\n",
    "This notebook builds multiple **recommender systems** based on **six different collaboartive filtering (CF) techniques** and compares these models against each other based on their performance on a dataset containing student performance data (i.e., a dataset containing the scores of students on assessment questions). Our chosen CF models include **three latent factor-based models** (Singular Value Decomposition, Singular Value Decomposition Plus, Non-Negative Matrix Factorization) and **three neighborhood-based models** (k-Nearest Neighbors, k-Nearest Neighbors with Means, k-Nearest Neighbors with Z-Scores).\n",
    "\n",
    "To evaluate whether CF-based recommender systems can effectively predict students' performance scores on new, unseen questions based on their past performance, this notebook conducts **[Dietterich's 5x2 CV paired t-test](https://pubmed.ncbi.nlm.nih.gov/9744903/)** on each model. We compare the models across two metrics, **Mean Absolute Error (MAE)** and **Root Mean Squared Error (RMSE)**, using two different datasets. For each model, we computed the average MAE and RMSE values from the 10 trials of the 5x2 CV process. To assess the statistical significance of the performance differences between each CF model and the baseline, we perform **two-sided paired t-tests** and calculate **Bonferroni-corrected p-values**, along with **Cohen's d** to measure effect sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Set dataset-specific variables\n",
    "After setting the following dataset-specific variables, you should be able to run this notebook without any additional changes.\n",
    "\n",
    "**NOTE**: This notebook assumes that the student performance dataset is stored as CSV file with one column for the (anonymized) student ID, one column for the question ID, and one column for the **normalized** score (i.e., a score falling between 0.0 and 1.0). If your dataset does not follow these specifications, you will also need to change the implementation of the ``load_and_preprocess_data`` function accordingly based on the shape of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the values for these variables before running the remaining cells of this notebook!\n",
    "\n",
    "# a string that specifies the path to the performance dataset from the current directory\n",
    "dataset_path = './datasets/dataset_2022.csv'\n",
    "# a string that specifies the NAME of the column containing the (anonymized) student IDs \n",
    "student_id_col_name = 'User ID'\n",
    "# a string that specifies the NAME of the column containing the question IDs \n",
    "question_id_col_name = 'Question ID'\n",
    "# a string that specifies the NAME of the column containing the normalized performance scores\n",
    "score_col_name = 'Score'\n",
    "\n",
    "# set to False if you want to disable status messages during model evaluation\n",
    "include_status_messages = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Import packages\n",
    "We use the [Surprise](https://surpriselib.com/) package, a Python scikit for building and analyzing CF-based recommender systems, to build and evaluate our CF models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "from surprise import Reader, Dataset\n",
    "from surprise import AlgoBase, SVD, SVDpp, NMF, KNNBasic, KNNWithMeans, KNNWithZScore\n",
    "from surprise.model_selection import KFold, cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Load and preprocess raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(path):\n",
    "    \"\"\"\n",
    "    Loads the performance dataset from the CSV file located at path.\n",
    "    \n",
    "    @param path: path to the performance dataset from the current directory\n",
    "    @return data_df: Pandas dataframe containing the loaded performance dataset\n",
    "    @return data_wrapped: Surprise dataframe containing the loaded performance dataset\n",
    "    \"\"\"\n",
    "    data_df = pd.read_csv(path, keep_default_na=False)\n",
    "    \n",
    "    # rename columns to match the names expected by the functions in the Surprise package\n",
    "    data_df = data_df.rename(\n",
    "        columns={question_id_col_name:'itemID', student_id_col_name:'userID', score_col_name:'rating'})\n",
    "    data_df = data_df[['itemID', 'userID', 'rating']]\n",
    "    data_df['rating'] = pd.to_numeric(data_df['rating']).fillna(0)\n",
    "    \n",
    "    # functions in the Suprise package require the data to be wrapped by a Surprise wrapper class\n",
    "    reader = Reader(rating_scale=(0.0, 1.0))\n",
    "    data_wrapped = Dataset.load_from_df(data_df[['userID', 'itemID', 'rating']], reader)\n",
    "    \n",
    "    return data_df, data_wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df, data_wrapped = load_and_preprocess_data(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the dataset was loaded properly\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the number of students and questions in the dataset\n",
    "num_students = len(set(data_df['userID']))\n",
    "num_questions = len(set(data_df['itemID']))\n",
    "print('Number of distinct  students in dataset: %d' % num_students)\n",
    "print('Number of distinct questions in dataset: %d' % num_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_per_student = data_df.groupby('userID').size().reset_index(name='Record Count')\n",
    "records_per_student = records_per_student.sort_values('Record Count', ascending=False)\n",
    "print(f\"Average records per student: {records_per_student['Record Count'].mean():.0f}\")\n",
    "print(f\"Median  records per student: {records_per_student['Record Count'].median():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Implement a baseline model\n",
    "We compare the performance of our CF models against an **average-based baseline model**, a standard benchmark in recommender system evaluations. For a given student and a new question, the baseline model predicts a performance score based on the average of three means: the overall mean score, the mean score of the student, and the mean score of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgBaseline(AlgoBase):\n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self, random_state=0)\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        \"\"\"\n",
    "        Fits the average-based model to the provided training set.\n",
    "\n",
    "        @param trainset: training set (wrapped by Surprise wrapper class) \n",
    "        \"\"\"\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        self.avg_rating = np.mean([r for (_, _, r) in self.trainset.all_ratings()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        \"\"\"\n",
    "        Predicts the score of user/student u on item/student i. \n",
    "\n",
    "        @param u: ID of the user/student\n",
    "        @param i: ID of the item/question\n",
    "        @return: the predicted score of user/student u on item/student i\n",
    "        \"\"\"\n",
    "        sum_means = self.avg_rating \n",
    "        div = 1\n",
    "        if self.trainset.knows_user(u):\n",
    "            sum_means += np.mean([r for (_, r) in self.trainset.ur[u]])\n",
    "            div += 1\n",
    "        if self.trainset.knows_item(i):\n",
    "            sum_means += np.mean([r for (_, r) in self.trainset.ir[i]])\n",
    "            div += 1\n",
    "\n",
    "        return sum_means / div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Build and evaluate models\n",
    "We follow [Dietterich's 5x2 CV technique](https://pubmed.ncbi.nlm.nih.gov/9744903/) to evaluate each of our models across the two benchmarking metrics of Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). This results in 5 iterations of 2-fold cross-validation for each model, giving us a total of 10 trials per model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['baseline', 'svd', 'svdpp', 'nmf', 'knn_basic', 'knn_means', 'knn_zscore']\n",
    "num_cf_models  = 6 # set to 6 for Bonferroni correction of p-values\n",
    "num_iterations = 5 # set to 5 for Dietterich's 5x2 CV test \n",
    "num_splits = 2     # set to 2 for Dietterich's 5x2 CV test \n",
    "num_trials = 10    # set to 10 since we have 5 iterations of 2-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_wrapped, kfold):\n",
    "    \"\"\"\n",
    "    Performs cross-validation on the specified model instance using MAE and RMSE as its measures.\n",
    "    \n",
    "    @param model: model instance to which cross-validation is applied\n",
    "    @param data_wrapped: dataset that will be used for cross-validation (wrapped by a Surprise class)\n",
    "    @param kfold: KFold object specifying the number of splits to use for cross-validation\n",
    "    @return results: dictionary containing the MAE and RMSE values for each testset from cross-validation.\n",
    "    \"\"\"\n",
    "    cv_results = cross_validate(model, data_wrapped, measures=['MAE', 'RMSE'], cv=kfold, verbose=include_status_messages)\n",
    "    results = {\n",
    "        'MAE': cv_results['test_mae'],\n",
    "        'RMSE': cv_results['test_rmse']\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalute all the models using Dietterich's 5x2 CV technique \n",
    "models_results = {\n",
    "    'baseline': [],\n",
    "    'svd': [],\n",
    "    'svdpp': [],\n",
    "    'nmf': [],\n",
    "    'knn_basic': [],\n",
    "    'knn_means': [],\n",
    "    'knn_zscore': []\n",
    "}\n",
    "for i in range(num_iterations):\n",
    "    print('\\n** ITERATION ROUND %d **' %(i+1))\n",
    "    random.seed(i)                                                                    \n",
    "    np.random.seed(i)\n",
    "    kfold = KFold(n_splits=num_splits, random_state=i)\n",
    "    \n",
    "    models_results['baseline'].append(evaluate_model(AvgBaseline(), data_wrapped, kfold))\n",
    "    models_results['svd'].append(evaluate_model(SVD(random_state=i), data_wrapped, kfold))\n",
    "    models_results['svdpp'].append(evaluate_model(SVDpp(random_state=i), data_wrapped, kfold))\n",
    "    models_results['nmf'].append(evaluate_model(NMF(random_state=i), data_wrapped, kfold))\n",
    "    models_results['knn_basic'].append(evaluate_model(KNNBasic(), data_wrapped, kfold))\n",
    "    models_results['knn_means'].append(evaluate_model(KNNWithMeans(), data_wrapped, kfold))\n",
    "    models_results['knn_zscore'].append(evaluate_model(KNNWithZScore(), data_wrapped, kfold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Compare models\n",
    "We report the mean MAE and RMSE values (from the 10 trials of the 5x2 CV process) for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_means(models_results, model_name):\n",
    "    \"\"\"\n",
    "    Computes and prints the mean MAE and RMSE values (from the 10 trials of the 5x2 CV process) for each model.\n",
    "    \n",
    "    @param model_results: the dictionary generated in the evaluation step containing the MAE and RMSE values\n",
    "                          for all the models across all iterations and splits\n",
    "    @param model_name: name of the model for which the mean MAE and RMSE will be computed\n",
    "    \"\"\"\n",
    "    print(f'\\n** RESULTS FOR {model_name.upper()} **')\n",
    "    for metric_name in ['MAE', 'RMSE']:\n",
    "        vals = np.array([])\n",
    "        for i in range(num_iterations):\n",
    "            vals = np.append(vals, models_results[model_name][i][metric_name])\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        print(f'Mean of {metric_name}: {mean} +/- {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    compute_means(models_results, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the statistical significance of the performance differences between each CF model and the average-based baseline, we perform a two-sided paired t-test using the 5x2 CV approach (as outlined in [Dietterich's paper](https://pubmed.ncbi.nlm.nih.gov/9744903/) under **Section 3.5 - The 5x2cv paired t-test**), with the assumption that the t-stat approximately follows a t-distribution with 5 degrees of freedom and a null hypothesis that both models 1 and 2 have equal performance. In addition to reporting the raw p-values, we also report the **Bonferroni-corrected p-values**. The Bonferroni correction, which multiplies the raw p-values by the number of tests conducted, is widely used when conducting multiple statistical tests, as it reduces the risk of false positives by adjusting p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_ttest(models_results, model1_name, model2_name):\n",
    "    \"\"\"\n",
    "    Computes and prints the p-values resulting form the comparison of models 1 and 2 across MAE and RMSE.\n",
    "    \n",
    "    @param model_results: the dictionary generated in the evaluation step containing the MAE and RMSE values\n",
    "                          for all the models across all iterations and splits\n",
    "    @param model1_name: name of the 1st model to be used in the paired t-test\n",
    "    @param model2_name: name of the 2nd model to be used in the paired t-test\n",
    "    \"\"\"\n",
    "    print(f'\\n** RESULTS FOR COMPARING {model1_name.upper()} AND {model2_name.upper()} **')\n",
    "    for metric_name in ['MAE','RMSE']:\n",
    "        perf_diff_var_sum = 0\n",
    "        for i in range(num_iterations):\n",
    "            perf_diff = models_results[model1_name][i][metric_name] - models_results[model2_name][i][metric_name]\n",
    "            perf_diff_mean = np.mean(perf_diff)\n",
    "            perf_diff_var = np.sum((perf_diff - perf_diff_mean)**2)\n",
    "            perf_diff_var_sum += perf_diff_var\n",
    "\n",
    "        perf_diff_first = models_results[model1_name][0][metric_name] - models_results[model2_name][0][metric_name]\n",
    "        t_stat = perf_diff_first[0] / np.sqrt(1/num_iterations*perf_diff_var_sum)\n",
    "        p_val = 2*(1 - t.cdf(abs(t_stat), num_iterations))\n",
    "\n",
    "        print(f'\\nRaw p-value based on {metric_name}: {p_val}')\n",
    "        print(f'Bonferroni-adjusted p-value based on {metric_name}: {p_val*num_cf_models}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    if model_name != 'baseline':\n",
    "        paired_ttest(models_results, model_name, 'baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to quantify the effect sizes of the performance differences between each CF model and the baseline, we calculate Cohen's d for each test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohen_d(models_results, model1_name, model2_name):\n",
    "    \"\"\"\n",
    "    Computes and prints the Cohen's d resulting form the comparison of models 1 and 2 across MAE and RMSE.\n",
    "    \n",
    "    @param model_results: the dictionary generated in the evaluation step containing the MAE and RMSE values\n",
    "                          for all the models across all iterations and splits\n",
    "    @param model1_name: name of the 1st model to be used in the effect size computation\n",
    "    @param model2_name: name of the 2nd model to be used in the effect size computation\n",
    "    \"\"\"\n",
    "    print(f'\\n** RESULTS FOR COMPARING {model1_name.upper()} AND {model2_name.upper()} **')\n",
    "    for metric_name in ['MAE','RMSE']:\n",
    "        vals1 = np.array([])\n",
    "        vals2 = np.array([])\n",
    "        for i in range(num_iterations):\n",
    "            vals1 = np.append(vals1, models_results[model1_name][i][metric_name])\n",
    "            vals2 = np.append(vals2, models_results[model2_name][i][metric_name])\n",
    "        \n",
    "        mean1 = np.mean(vals1)\n",
    "        mean2 = np.mean(vals2)\n",
    "        std1 = np.std(vals1)\n",
    "        std2 = np.std(vals2)\n",
    "        \n",
    "        pooled_std = std1 + std2 / 2\n",
    "        cohen_d = np.abs(mean1 - mean2) / pooled_std\n",
    "\n",
    "        print(f'Cohens d based on {metric_name}: {cohen_d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    if model_name != 'baseline':\n",
    "        cohen_d(models_results, model_name, 'baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Evaluate Models Across Data Availability Levels\n",
    "\n",
    "In this section, we evaluate all six collaborative filtering models and the baseline model across 20% using the same Dietterich's 5x2 CV technique. This allows us to assess how model performance compares when differents levels of data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_subset(data_df, availability_percentage):\n",
    "    subset_data = []\n",
    "    \n",
    "    for student_id in data_df['userID'].unique():\n",
    "        student_records = data_df[data_df['userID'] == student_id]\n",
    "        num_records = len(student_records)\n",
    "        num_to_keep = max(1, int(num_records * availability_percentage / 100))\n",
    "        \n",
    "        sampled_records = student_records.sample(n=num_to_keep, random_state=0)\n",
    "        subset_data.append(sampled_records)\n",
    "    \n",
    "    subset_df = pd.concat(subset_data, ignore_index=True)\n",
    "    return subset_df\n",
    "\n",
    "data_availability_levels = {\n",
    "    '20%': create_data_subset(data_df, 20)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "availability_levels = [20]\n",
    "\n",
    "availability_results = {\n",
    "    '20%': {'baseline': [], 'svd': [], 'svdpp': [], 'nmf': [], 'knn_basic': [], 'knn_means': [], 'knn_zscore': []},\n",
    "}\n",
    "\n",
    "availability_data = {}\n",
    "for availability in availability_levels:\n",
    "    subset_df = create_data_subset(data_df, availability)\n",
    "    availability_key = f'{availability}%'\n",
    "    \n",
    "    reader = Reader(rating_scale=(0.0, 1.0))\n",
    "    availability_data[availability_key] = Dataset.load_from_df(\n",
    "        subset_df[['userID', 'itemID', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for availability_key in ['20%']:\n",
    "    print(f'\\n\\n===== EVALUATING MODELS WITH {availability_key} DATA AVAILABILITY =====')\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        print(f'\\n** ITERATION ROUND {i+1} **')\n",
    "        random.seed(i)                                                                    \n",
    "        np.random.seed(i)\n",
    "        kfold = KFold(n_splits=num_splits, random_state=i)\n",
    "        \n",
    "        # Evaluate all models on this availability level\n",
    "        availability_results[availability_key]['baseline'].append(\n",
    "            evaluate_model(AvgBaseline(), availability_data[availability_key], kfold))\n",
    "        availability_results[availability_key]['svd'].append(\n",
    "            evaluate_model(SVD(random_state=i), availability_data[availability_key], kfold))\n",
    "        availability_results[availability_key]['svdpp'].append(\n",
    "            evaluate_model(SVDpp(random_state=i), availability_data[availability_key], kfold))\n",
    "        availability_results[availability_key]['nmf'].append(\n",
    "            evaluate_model(NMF(random_state=i), availability_data[availability_key], kfold))\n",
    "        availability_results[availability_key]['knn_basic'].append(\n",
    "            evaluate_model(KNNBasic(), availability_data[availability_key], kfold))\n",
    "        availability_results[availability_key]['knn_means'].append(\n",
    "            evaluate_model(KNNWithMeans(), availability_data[availability_key], kfold))\n",
    "        availability_results[availability_key]['knn_zscore'].append(\n",
    "            evaluate_model(KNNWithZScore(), availability_data[availability_key], kfold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Summary by Data Availability Level\n",
    "\n",
    "We report the mean MAE and RMSE values for each model at 20% data availability level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_means_by_availability(availability_results, availability_key):\n",
    "    \"\"\"\n",
    "    Computes and prints the mean MAE and RMSE values for all models at a given data availability level.\n",
    "    \n",
    "    @param availability_results: dictionary containing results for each availability level and model\n",
    "    @param availability_key: the data availability level\n",
    "    \"\"\"\n",
    "    print(f'\\n\\n===== RESULTS FOR {availability_key} DATA AVAILABILITY =====')\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        print(f'\\n** {model_name.upper()} **')\n",
    "        for metric_name in ['MAE', 'RMSE']:\n",
    "            vals = np.array([])\n",
    "            for i in range(num_iterations):\n",
    "                vals = np.append(vals, availability_results[availability_key][model_name][i][metric_name])\n",
    "            mean = np.mean(vals)\n",
    "            std = np.std(vals)\n",
    "            print(f'Mean {metric_name}: {mean:.6f} +/- {std:.6f}')\n",
    "\n",
    "# Compute results for each availability level\n",
    "for availability_key in ['25%', '50%', '75%']:\n",
    "    compute_means_by_availability(availability_results, availability_key)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
